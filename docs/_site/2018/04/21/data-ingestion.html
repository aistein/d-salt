<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Data Ingestion | D-SALT</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Data Ingestion" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Using the recommended data ingestion pipelines can improve gpu utilization from 30% to 61% in simple models, and decrease training time by 50%" />
<meta property="og:description" content="Using the recommended data ingestion pipelines can improve gpu utilization from 30% to 61% in simple models, and decrease training time by 50%" />
<link rel="canonical" href="http://localhost:4000/2018/04/21/data-ingestion.html" />
<meta property="og:url" content="http://localhost:4000/2018/04/21/data-ingestion.html" />
<meta property="og:site_name" content="D-SALT" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-04-21T00:00:00-04:00" />
<script type="application/ld+json">
{"description":"Using the recommended data ingestion pipelines can improve gpu utilization from 30% to 61% in simple models, and decrease training time by 50%","@type":"BlogPosting","url":"http://localhost:4000/2018/04/21/data-ingestion.html","headline":"Data Ingestion","dateModified":"2018-04-21T00:00:00-04:00","datePublished":"2018-04-21T00:00:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/04/21/data-ingestion.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="D-SALT" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">D-SALT</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Data Ingestion</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-04-21T00:00:00-04:00" itemprop="datePublished">Apr 21, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In Tensorflow the majority of users are most familiar with what is known as Feeding data. The Tensorflow feed mechanism allows users to inject data into any Tensor in their computational graph. Here is a simple example (<a href="https://www.tensorflow.org/api_guides/python/reading_data">original source</a>).</p>

<h2 id="build-and-profile-a-model">Build And Profile A Model</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">():</span>
  <span class="nb">input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">classifier</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">print</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="nb">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="nb">input</span><span class="p">:</span> <span class="n">my_python_preprocessing_fn</span><span class="p">()}))</span>
</code></pre></div></div>
<p>We have built a simple model based on <a href="https://arxiv.org/pdf/1701.04783.pdf">this</a> paper that uses a feed dict to train. You can find the source code for our model <a href="https://github.com/aistein/dlprof/blob/master/DeepCoNN%20-%20feed%20dict.ipynb">here</a>. There are two important aspects to this model.</p>

<p>First, you will see that we are using a <code class="highlighter-rouge">feed_dict</code> to run an interative optimization operation.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">train_op</span><span class="p">,</span> <span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
    <span class="n">u_inputs</span><span class="p">:</span> <span class="n">user_batch</span><span class="p">,</span>
    <span class="n">i_inputs</span><span class="p">:</span> <span class="n">item_batch</span><span class="p">,</span>
    <span class="n">ratings_input</span><span class="p">:</span> <span class="n">rating_batch</span>
</code></pre></div></div>
<p>Second, note that the <code class="highlighter-rouge">Batch_Dataset</code> class is iterable, and that it has the <code class="highlighter-rouge">__next__</code> function defined.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Batch_Dataset</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="nb">iter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="nb">iter</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_iter</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">raise</span> <span class="nb">StopIteration</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rand_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ratings</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">user_review_list</span><span class="p">[</span><span class="n">rand_indices</span><span class="p">]),</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">item_review_list</span><span class="p">[</span><span class="n">rand_indices</span><span class="p">]),</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ratings</span><span class="p">[</span><span class="n">rand_indices</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>
<p>As defined our <code class="highlighter-rouge">__next__</code> method will select a random subset from three numpy arrays, one containing user reviews of items, one containing reviews for an item by other users, and one containing the rating this user gave this item. It is important to note that this is text data that has already been cleaned and only needs to be converted to indices, then embeddings, in order to pass them through our network.</p>

<p>We use tensorflow to do the conversion by defining a HashTable and embeddings Variable.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">table</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">HashTable</span><span class="p">(</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">KeyValueTensorInitializer</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
    <span class="s">"word_embeddings"</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">),</span> <span class="n">emb_size</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>
<p>We found that this model took <code class="highlighter-rouge">59.807</code> seconds to train to completion. Further, we found that the GPU reached at most <code class="highlighter-rouge">30%</code> utilization.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      2027      C   python                                     10941MiB |
+-----------------------------------------------------------------------------+
Sat Apr 21 22:34:11 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   63C    P0    65W / 149W |  10954MiB / 11441MiB |     30%      Default |
+-------------------------------+----------------------+----------------------+
</code></pre></div></div>

<h2 id="getting-the-most-out-of-tensorflow">Getting The Most Out Of Tensorflow</h2>

<p>For our case this may be sufficient. Waiting only one minute for a model to train is amazing compared to other models like AlphaGo which take 4 - 6 weeks even with all the resources available to DeepMind. According to the Tensorflow documentation <a href="https://www.tensorflow.org/performance/performance_guide">here</a></p>

<blockquote>
  <p>If GPU utilization is not approaching 80-100%, then the input pipeline may be the bottleneck.</p>
</blockquote>

<p>To see what may be holding us back from full gpu utilization we decided to run a check with the python profiler.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python -m cProfile -s tottime DeepCoNN\ -\ feed\ dict.py &gt; profile.txt
</code></pre></div></div>

<p>Looking inside profile.txt we find the following.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Ordered by: internal time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      825   17.388    0.021   17.388    0.021 {built-in method _pywrap_tensorflow_internal.TF_Run}
    58294   15.318    0.000   15.318    0.000 {built-in method numpy.core.multiarray.array}
    ...
</code></pre></div></div>
<p>As expected, our model spends the most time on some Tensorflow <code class="highlighter-rouge">TF_Run</code> which takes approximately 17.388 seconds. Interestingly, the next line shows us that the second most expensive method is a cast to Numpy arrays which almost takes just as long as the <code class="highlighter-rouge">TF_Run</code> calls. This begs the question, how do we remove this cast to numpy arrays?</p>

<p>Of course, Tensorflow has a built in api for <a href="https://www.tensorflow.org/programmers_guide/datasets">Importing Data</a>. They even have a <a href="https://www.tensorflow.org/performance/datasets_performance">performance guide</a>, which we will use as a roadmap to change our model’s training process.</p>

<p>We can envision the problem with our model with the following image, taken from the Tensorflow performance guide link.</p>

<p><img src="/dlprof/assets/datasets_without_pipelining.png" alt="No Pipelining" /></p>

<p>Our problem is actually twofold. First, our model is spending too much time waiting for python to break the dataset into random batches of Numpy arrays. Additionally, though less obvious, we then have to transfer that data from Python’s environment to the Tensorflow session. Both of these problems can be solved simultaneously using Tensorflow’s <code class="highlighter-rouge">tf.data</code> api. This api can ingest data from multiple file types like csv, text files, string inputs and even multiple files of those types. We will use <code class="highlighter-rouge">tfrecords</code> because they are described as the standard tensorflow format. For now we will assume you have your data in a binary <code class="highlighter-rouge">*.tfrecords</code> file of the proper format, but due to a lack of documentation, we will be making another, shorter, post on how to create these data files and work with the api.</p>

<p>Additionally, we will change our model to use the Estimator api to ingest data in order to handle variable, queue, and table initialization as well as removing the call to <code class="highlighter-rouge">tf_run</code> and <code class="highlighter-rouge">feed_dict</code> usage. You can find the final model <a href="https://github.com/aistein/dlprof/blob/master/DeepCoNN%20-%20tfrecords.ipynb">here</a>. Importantly, you will note that we have defined our model construction in a single function with <code class="highlighter-rouge">model_fn(features, labels, mode)</code>. This model is fed data through the <code class="highlighter-rouge">features</code> parameter via an iterator built by the following function.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_dataset_iterator</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">pad_value</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TFRecordDataset</span><span class="p">(</span><span class="n">loc</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="n">parse_fn</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="n">split_fn</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="n">get_truncate_fn</span><span class="p">(</span><span class="n">max_len</span><span class="p">),</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">padded_shapes</span><span class="o">=</span><span class="p">([</span><span class="n">max_len</span><span class="p">],</span> <span class="p">[</span><span class="n">max_len</span><span class="p">],</span> <span class="p">[</span><span class="bp">None</span><span class="p">]),</span> <span class="n">padding_values</span><span class="o">=</span><span class="p">(</span><span class="n">pad_value</span><span class="p">,</span> <span class="n">pad_value</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">))</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">26352</span><span class="p">)</span>
    <span class="n">iterator</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">make_one_shot_iterator</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">iterator</span>
</code></pre></div></div>
<p>As we can see, this reads a tfrecords file from the location, prefetches a batch, parses the data, splits the data (we are working with strings), truncates strings that are too long, pads and batches strings that are too short, shuffles the full dataset (in our case we know there are 26352 data points) then returns an iterator that traverses each data point. For now we will assume some familiarity with reading tfrecord files, but in another post we will have more documentation on doing so.</p>

<p>By using tfrecords and the <code class="highlighter-rouge">tf.data</code> api our model is able to train in 30.879 seconds. Again, think in percentages of improvement rather than absolute terms, our 30 second improvement is actually 50% less time to train the model. This could be a 4 - 6 week training time turning into a 2 - 3 week training time. Looking at our GPU utilization, we actually got as high as 61% at some points! Approaching the 80% utilization the Tensorflow documentation specifies as an approximate heuristic.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      2101      C   python                                     10941MiB |
+-----------------------------------------------------------------------------+
Sat Apr 21 22:35:18 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   66C    P0   106W / 149W |  10954MiB / 11441MiB |     61%      Default |
+-------------------------------+----------------------+----------------------+
</code></pre></div></div>

<p>Finally looking at the python profile we can obviously see that our cast to numpy is gone, and the most time consuming process was <code class="highlighter-rouge">TF_Run</code> which took about 24 seconds because it now includes all of our data ingestion as well as model training.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Ordered by: internal time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      851   24.341    0.029   24.341    0.029 {built-in method _pywrap_tensorflow_internal.TF_Run}
</code></pre></div></div>

<p>What we can imagine having happening now is that the Tensorflow api is able to read and preprocess data prior to our model requesting more data to train on. Rather than all the time spent idle as above, our model training now looks like the bottom diagram in the following image during each iteration.</p>

<p><img src="/dlprof/assets/datasets_parallel_map.png" alt="Parallel Data Ingestion" /></p>

<p>In conclusion, inference and model training times have opportunity for vast improvement using simple, well known tools and concepts. Simply using the recommended input format, parallelizing data collection and preprocessing improved our training time by 50% which in long-running models is significant.</p>

  </div><a class="u-url" href="/2018/04/21/data-ingestion.html" hidden></a>
</article>

      </div>
    </main>
<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">D-SALT</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">D-SALT</li>
            <li><a class="u-email" href="mailto:as5281@columbia.edu">as5281@columbia.edu</a></li>
            <li><a class="u-email" href="mailto:kvm2116@columbia.edu">kvm2116@columbia.edu</a></li>
            <li><a class="u-email" href="mailto:pck2119@columbia.edu">pck2119@columbia.edu</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/aistein"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">aistein</span></a></li>
</ul>
</div>

      <div class="footer-col footer-col-3">
        <p>D-SALT: Datacenter Sender Adaptive Low-Latency Transport</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
